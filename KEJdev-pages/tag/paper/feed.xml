<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="https://kejdev.github.io/tag/paper/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://kejdev.github.io/" rel="alternate" type="text/html" />
  <updated>2019-12-26T17:15:56+09:00</updated>
  <id>https://kejdev.github.io/tag/paper/feed.xml</id>

  
  
  

  
    <title type="html">Py귤의 판교 생존기 | </title>
  

  
    <subtitle>developer log ..</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">논문요약, Predicting remaining useful life of rolling bearings based on deep feature representation and long short-term memory neural network</title>
      <link href="https://kejdev.github.io/Deep-feature-lstm-rul" rel="alternate" type="text/html" title="논문요약, Predicting remaining useful life of rolling bearings based on deep feature representation and long short-term memory neural network" />
      <published>2019-12-17T19:00:00+09:00</published>
      <updated>2019-12-17T19:00:00+09:00</updated>
      <id>https://kejdev.github.io/Deep-feature-lstm-rul</id>
      <content type="html" xml:base="https://kejdev.github.io/Deep-feature-lstm-rul">&lt;h3 id=&quot;predicting-remaining-useful-life-of-rolling-bearings-based-on-deep-feature-representation-and-long-short-term-memory-neural-network&quot;&gt;Predicting remaining useful life of rolling bearings based on deep feature representation and long short-term memory neural network&lt;/h3&gt;

&lt;p&gt;Paper URL: &lt;a href=&quot;https://journals.sagepub.com/doi/full/10.1177/1687814018817184&quot;&gt;https://journals.sagepub.com/doi/full/10.1177/1687814018817184&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;논문 이름이 엄청 길지만 사실 그렇게 심오하지 않는 논문에 대해 요약 리뷰를 해보겠습니다. 이 논문은&lt;strong&gt;deep feature extraction&lt;/strong&gt;에 관련된 논문입니다. 신경망쪽으로 조금 알고 계신분이라면 아마 &lt;strong&gt;deep feature extraction&lt;/strong&gt;이 무엇인지는 알고 계실테지만 밑에서 한번 짚고 넘어가겠습니다. 또한 이번 논문은 RUL과 관련이 있기 때문에 RUL이 무엇인지도 알아보겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;rulremaining-useful-life&quot;&gt;RUL(remaining useful life)&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;사실 &lt;strong&gt;RUL&lt;/strong&gt;이라는 단어가 사실 정식 단어인지 아닌지는 잘 모르겠습니다. RUL은 어떠한 시스템이나 구성 요소가 교체하기 전에 의도한 목적에 따라 작동 할 수 있을 것으로 추정되는 &lt;strong&gt;잔여 수명&lt;/strong&gt;을 이야기합니다. 즉, 어떠한 부품이 있을때, 이 부품의 남은 수명 같은 개념입니다.&lt;/p&gt;

&lt;p&gt;이 논문은 이러한 RUL, 즉 잔존 수명이 얼마나 남았는지를 예측하기 위한 신경망 모델에 대한 논문이며 CNN과 LSTM을 사용하여 모델을 만들었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;deep-feature&quot;&gt;Deep feature&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;deep feature 간단하게 말하면 신경망에서 나오는 feature를 이야기 합니다. 아래의 이미지에서 사람의 사진이 신경망을 거치면서 나오는 저 feature들을 이야기 합니다. 
(이미지 출처는 &lt;a href=&quot;ttps://youthassembly.or.kr/bbs/board.php?bo_table=B56&amp;amp;wr_id=30225&quot;&gt;여기&lt;/a&gt;를 클릭해주세요.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/feature.png&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;
&lt;center&gt; edges들이 feature에 해당됨 &lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;신경망이 학습을 하면서 이미지에서 feature가 추출 되는데 우리는 이것을 &lt;strong&gt;deep feature&lt;/strong&gt;이라고 이야기 합니다. 이러한 deep feature extraction(추출)은 신경망이 학습하면서 좋은 feature를 추출하면 할수록 더 좋은 성능을 뽑게 됩니다. 좋은 feature가 있다는 것은 그 많큼 좋은 성능의 Model이라고도 볼 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;예를 들어 사과와 바나나를 구별할 때, 사과는 동그랗다 라는 것을 알수 있는 feature와 바나나는 길다 라는 것을 알수 있는 feature를 위주로 뽑아가며 학습을 한다고 가정을 해봅시다. 이 feature들이 뚜렷하고 정확하면 할수록 분류를 잘하기 때문에 좋은 모델이라고도 애기할 수 있습니다.&lt;/p&gt;

&lt;p&gt;반대로 이야기 하자면, 좋은 feature를 데이터 Input 값으로 넣는다면 좋은 모델을 뽑을수도 있다는 말로도 바꿀 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 점들을 이용해서 요즘은 여러 모델을 섞어서 쓰기도 합니다. 이 논문에서는 CNN과 LSTM을 가지고 모델을 만들었으며 이 논문의 모델의 핵심이 &lt;strong&gt;deep feature extraction&lt;/strong&gt;이기 때문에 deep feature을 짚고 넘어가보았습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;dataset&quot;&gt;DataSet&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;논문에서 사용된 데이터는 PHM IEEE 2012 Challenge에서 사용된 데이터입니다. PHM 데이터는 진동 데이터입니다. 진동 데이터를 사용한 이유는 수명이 점점 다해가면서 사용 중인 기계의 진동이 점점 강해진다는 점을 파악하여 만들어진 데이터 입니다. 공장에서는 큰 기계를 주로 사용하는데, 고장이나, 수명을 쉽게 파악하기 위해 진동 센서를 붙여 진동의 세기 등을 보고 파악한다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/phm_data.png&quot; width=&quot;420&quot; height=&quot;230&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터는 위와 같습니다. 베어링 부품의 진동을 수집하였고 수집 상태는 총 3가지로 나눌 수 있으며, 아래와 같습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First operating conditions: 1800 rpm and 4000 N&lt;/li&gt;
  &lt;li&gt;Second operating conditions: 1650 rpm and 4200 N&lt;/li&gt;
  &lt;li&gt;Third operating conditions: 1500 rpm and 5000 N&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;데이터 상세 컬럼은 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/column.png&quot; width=&quot;620&quot; height=&quot;80&quot; /&gt;&lt;/p&gt;

&lt;p&gt;시간, 분, 초 등으로 이루어진 데이터라는 것을 알 수 있으며, 데이터 다운로드는 &lt;a href=&quot;https://github.com/wkzs111/phm-ieee-2012-data-challenge-dataset&quot;&gt;여기&lt;/a&gt;에서 보실 수 있고 데이터 상세 설명은 PDF로 같이 있으니 참고 하시길 바랍니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;data-processing&quot;&gt;Data processing&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;첫 번째 데이터 전처리 단계에서 원본 데이터를 Hilbert Huang transform(HHT)로 변환합니다. 데이터 변환 후 SVD라고 하여 상관 계수를 구하는 단계를 거치고 데이터를 Input으로 CNN모델에 넣어줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fft.png&quot; width=&quot;500&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;HHT는 FFT와는 조금 다르니, 위 이미지(흐릿하지만)를 보고 참고 하세요. (a)가 원본 데이터, (b)가 FFT, (c)가 HHT한 데이터 입니다. 더 선명한 이미지는 논문에 있으니 참고하면 될 것 같습니다. 두 번째 전처리 단계는 CNN에서 학습이 끝나면 바로 전 레이어인, full-connected layer 에서 feature를 PCA로 돌려서 4까지 뽑고 LSTM의 Input으로 넣었습니다. 데이터 전처리는 HHT와 SVD와 PCA 이 세가지를 했고 실제로 만들어서 돌려본 결과 HHT하는데 데이터 양이 많아서 그런지는 모르겠지만 시간이 무척이나 걸렸어요.. 시간을 조금 줄이고 싶으신 분은 사실 FFT만 하셔도 무난한 성능을 얻으실 수 있을 것 같습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;cnn_lstm&quot;&gt;CNN_LSTM&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 논문에서 만든 모델 Flowchart는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/flow.png&quot; width=&quot;350&quot; height=&quot;450&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터 전처리 후 CNN Model을 돌리는데, 정확도가 99%가 될때까지 학습을 돌리고, 99%가 되는 순간 마지막 레이어 full-connected layer에서 deep feature extraction을 하게 됩니다. 아마 이 논문에서 가장 중요한 부분이 이 부분 같습니다. 정확도가 높을 때, feature를 뽑는 이유는 제일 좋은 feature를 LSTM Input값으로 넣기 위함이겠죠.&lt;/p&gt;

&lt;p&gt;그런데 여기서 바로 feature를 뽑고 LSTM으로 넘기는게 아니라 PCA를 하게 됩니다. 주성분 분석으로 4까지 뽑은 후 LSTM의 Input으로 넣었습니다. CNN은 2layer, Kernel size는 2*2, feature maps 64, 128 입니다. 마지막으로 fun-connected는 25 neuron 입니다. 논문에서는 CNN Train acc 는 100%, Test acc 는 99.53%라고 합니다. LSTM의 파라미터는 자세히는 적혀 있지는 않지만 epoch 40이라고 하네요. 논문의 데이터는 시계열 데이터 이면서 RUL 예측 하기 위한 모델이기 때문에 평가방법은 RMSE를 사용했다고 합니다.&lt;/p&gt;

&lt;p&gt;사실 중간에 25개의 통계식과 함께 통계적으로 feature를 나타내어 비교하는 부분이 있으나, 신경망 구현에 있어서 건너뛰어도 무방하여 건너 뛰었습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;요약-및-핵심&quot;&gt;요약 및 핵심&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 논문에서는 CNN을 좋은 feature를 추출하기 위함으로 사용하였고 LSTM을 이용하여 시계열 데이터를 예측하였습니다. 이렇게 어떠한 모델에서 feature를 추출하는 것을 Deep feature extraction 이라고 부르며, RUL 평가방법으로 RMSE를 사용합니다.&lt;/p&gt;

&lt;p&gt;어렵게 볼 논문은 아닌지라, 간단하게 읽고 모델에 대한 영감 받기 좋은 논문인 것 같습니다. 논문보고 구현한 코드는 나중에 올리게 된다면 다시 업데이트 하겠습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>KEJdev</name>
        
        
      </author>

      

      
        <category term="PAPER" />
      

      
        <summary type="html">Predicting remaining useful life of rolling bearings based on deep feature representation and long short-term memory neural network</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">논문요약, Large-Scale Image Retrieval with Attentive Deep Local Features</title>
      <link href="https://kejdev.github.io/Attentive-Deep-Local-Features" rel="alternate" type="text/html" title="논문요약, Large-Scale Image Retrieval with Attentive Deep Local Features" />
      <published>2018-01-17T19:00:00+09:00</published>
      <updated>2018-01-17T19:00:00+09:00</updated>
      <id>https://kejdev.github.io/Attentive-Deep-Local-Features</id>
      <content type="html" xml:base="https://kejdev.github.io/Attentive-Deep-Local-Features">&lt;h3 id=&quot;large-scale-image-retrieval-with-attentive-deep-local-features&quot;&gt;Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/h3&gt;

&lt;p&gt;Paper URL: &lt;a href=&quot;https://arxiv.org/pdf/1612.06321.pdf&quot;&gt;https://arxiv.org/pdf/1612.06321.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이 논문은 CNN 계열의 DELF 라고 부르는 모델을 설명하는 논문입니다.&lt;br /&gt;
CNN( Convolutional Neural Network ) 모델이 무엇인지는 설명을 안해도 아시죠?&lt;br /&gt;
혹시 모르니, 나중에 CNN 모델을 더 자세하게 따로 다루겠습니다.&lt;/p&gt;

&lt;p&gt;CNN은 이미지 연구 분야에서 대단한 성과를 이뤘지만 CNN의 단점이 의외로 많답니다.
혹시 대규모의 이미지 검색 시스템에서는 성능이 무척이나 떨어진다는 것을 아시나요?&lt;/p&gt;

&lt;p&gt;실제로 CNN을 이용하여 방대한 이미지를 검색하는 시스템에서는 거의 사용하기 힘들다고 합니다. 그렇다면 CNN을 이용해서 방대한 이미지를 검색하려면 어떻게 해야댈까요 ?&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;the-main-purpose-of-the-paper&quot;&gt;The main purpose of the paper.&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 논문은 CNN을 활용한 대규모 이미지 검색 시스템을 만드는 것이 목적입니다.&lt;br /&gt;
논문에서는 CNN기반의 DELF 라는 모델을 만들었습니다.&lt;/p&gt;

&lt;p&gt;DELF모델은 아래의 그림과 같은 구조를 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/../assets/images/delf.png&quot; width=&quot;850&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모델의 구조에 대해서는 아래에서 천천히 알아볼께요.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;dataset&quot;&gt;Dataset&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;우선 논문에서 사용된 데이터는 구글에서 공개한 Landmark 데이터입니다.&lt;br /&gt;
이 데이터는 정말 방대한 데이터이면서 여태까지 공개되어 있는 그 어떤 Landmark 데이터보다 좋은 데이터라고  알려져 있습니다.
요즘 캐글이나 Landmark 데이터에 관해 검색을 조금만 하셔도 금방 데이터를 찾아볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;데이터는 12,849개의 명소가 포함된 총 1,060,709개의 이미지와 111,036개의 Query 이미지가 들어있습니다. 
전세계를 대상으로 추출되었고, GPS정보와 연계되어 있습니다.&lt;/p&gt;

&lt;p&gt;아래의 이미지는 데이터의 일부입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/../assets/images/(a)data.png&quot; width=&quot;550&quot; height=&quot;300&quot; /&gt;
&lt;img src=&quot;/../assets/images/(b)data.png&quot; width=&quot;550&quot; height=&quot;300&quot; /&gt;
&lt;img src=&quot;/../assets/images/(c)data.png&quot; width=&quot;550&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ground_truth&quot;&gt;Ground Truth&lt;/a&gt; 정보를 만들기 위해서 &lt;strong&gt;아래의 두가지 feature 정보를 활용&lt;/strong&gt;했습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Visual feature&lt;/li&gt;
  &lt;li&gt;GPS coordinates&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 2개의 정보를 활용하여 클러스터를 구축하였고 Query 이미지와 클러스터 거리가 특정 threshold 이내로 들어오면 동일한 이미지라고 판단합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;delf-model&quot;&gt;DELF Model&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;이제 DELF 모델에 대해 자세하게 설명하겠습니다.&lt;br /&gt;
아래의 그림은 맨 처음에 보여드린 DELF의 전체적인 구조입니다. 
기억나시나요 ?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/../assets/images/delf.png&quot; width=&quot;850&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DELF 모델의 기본적인 백본은 ResNet50입니다.&lt;br /&gt;
ResNet50을 사용한 이유는 &lt;a href=&quot;https://blog.naver.com/PostView.nhn?blogId=laonple&amp;amp;logNo=220958109081&amp;amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F&quot;&gt;FCN&lt;/a&gt; 정보를 활용하기위해서 사용했으며, pretrain 된 net에서 conv4_x를 사용했습니다.&lt;/p&gt;

&lt;p&gt;우선 논문에서는 image pyramid를 사용하여 여러개의 feature를 추출하였고, Landmark 데이터로 fine-tunnung 과정을 거칩니다. 
입력 이미지는 모두 center crop 한 뒤에 250x250으로 rescale 되는데, 여기서 또 244x244 크기로 랜덤 crop을 거칩니다.&lt;br /&gt;
(이 과정에서 object나 patch 레벨에서의 정보는 사용하지 않는다고 합니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/../assets/images/models.png&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;추출된 feature를 모두 사용하는 것이 아니라 Attention을 이용해서 keypoint만 추출 한다고 합니다.
keypoint를 추출하는 이유는 시스템 효율화에도 매우 중요하며, 요즘 Attention을 이용해 신경망을 만드는 것이 트렌드이니, 혹시나 Attention을 모르신다면, 이 기회에 알아보는 것도 좋겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;h4 id=&quot;learning&quot;&gt;Learning&lt;/h4&gt;
&lt;/blockquote&gt;

&lt;p&gt;논문에서는 weighted sum을 이용한 방식을 사용했으며,&lt;/p&gt;
&lt;center&gt;$$y=W(\sum _{ n } α(f_{ n };θ)⋅f_{ n })$$&lt;/center&gt;

&lt;p&gt;여기서 $α(f_{ n };θ)$는 score 함수입니다.&lt;br /&gt;
이때의 파라미터는 
이때의 파라미터는 $θ$가 되며 $W$는 $W∈R^{ Mxd }$ 이고 $M$는 클래스의 갯수입니다. Loss 는 cross entropy loss를 사용했습니다.&lt;/p&gt;
&lt;center&gt;$$L=−y∗⋅log(\frac { exp(y) }{  1^{T}exp(y)})$$&lt;/center&gt;

&lt;p&gt;여기서 $y*$는  ground-truth 이고 1 는 one vector입니다. 
실제 score는 2개의 conv 레이어와 softplus 비선형 함수로 구현되어 있으며, 더 자세한 설명은 논문을 참고하시길 바랍니다.&lt;/p&gt;

&lt;p&gt;제안된 DELF 모델은 정말 간단하게 &lt;strong&gt;요약&lt;/strong&gt;하자면, ature 와 attention 이 두가지가 모두 함게 학습되는 구조 입니다. 
그러나 이대로 학습을 한다면, 학습이 잘 안될 수 있어서 여기서는 2-step 학습 방식을 제안했습니다.&lt;/p&gt;

&lt;p&gt;간단합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;먼저 descriptor 학습&lt;/li&gt;
  &lt;li&gt;descriptor 를 고정하고 score-function 을 학습.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;최대한 성능을 올리기 위해 Attention 을 학습할 때 다양한 scale로 학습한다고 합니다. 또한 검색 성능을 올리기 위해 최종 dim 을 줄이는데, 먼저 L2 norm을 적용 후 PCA를 돌려 40dim까지 줄입니다. 그리고 다시 L2 norm 하면 끝이 납니다.&lt;/p&gt;

&lt;p&gt;이 논문의 평가방법은 일반적으로 사용하는 mAP( mean average precision ) 에서 약간 바꿔서 사용했습니다. mAP는 모든 쿼리마다 얻어진 결과를 relevance 순으로 정렬한뒤 측정된 ap의 평균값을 의미합니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 아래의 수식으로 바꾸어 평가하였습니다.&lt;/p&gt;
&lt;center&gt;$$Pre=\frac { ∑_{ q }|R_{ q }^{ TP }| }{ ∑_{ _{ q } }|R_{ q }| } ,Rec=\sum _{ q } |R_{ q }^{ TP }|$$&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
$R_{ q }$는 쿼리 q로 얻어진 검색 결과를 의미하며, $R_{ q }^{ TP }$는 true-positive 를 의미합니다.&lt;br /&gt;
아래의 표는 성능표입니다.  FT는 fine-tuning, ATT는 Attention 이라는 뜻입니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;/../assets/images/pred1.png&quot; width=&quot;550&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;/../assets/images/pred2.png&quot; width=&quot;550&quot; height=&quot;300&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;/../assets/images/pred3.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;표를 보면 좋은 성능의 모델이 무엇인지 잘 알겠죠 ?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;기본 CNN을 이용해서 Landmark 데이터 셋을 이용한다면, 좋은 정확도는 절대 뽑지 못할 거라 생각해요.&lt;/p&gt;

&lt;p&gt;이 논문을 선택한 이유는 방대한 데이터셋에서 빠르게 이미지를 찾기위해 논문을 찾다가 알게 되었고,
저는 이 논문을 기반으로 짠 코드를 Never AI 해커톤에 나갔으며, 결승까지 가게 되었습니다.&lt;/p&gt;

&lt;p&gt;이 논문을 기반으로 짠 코드가 궁금하시다면 &lt;a href=&quot;https://github.com/KEJdev/mandoo-model&quot;&gt;여기&lt;/a&gt;를 클릭해주세요.&lt;br /&gt;
다음에는 더 좋은 논문과 더 잘 요약된 포스팅글로 찾아뵐께요.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>KEJdev</name>
        
        
      </author>

      

      
        <category term="PAPER" />
      

      
        <summary type="html">Large-Scale Image Retrieval with Attentive Deep Local Features</summary>
      

      
      
    </entry>
  
</feed>
